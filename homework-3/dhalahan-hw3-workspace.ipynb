{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0275a5ef-eba9-4736-8a94-8cbf0c1b6905",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad04c1f6-83c3-4629-ac07-a8353b52268a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType, LongType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "import boto3\n",
    "from functools import reduce\n",
    "import pyarrow.parquet as pq\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "S3 = \"s3\"\n",
    "BUCKET_NAME = \"robot-dreams-source-data\"\n",
    "YELLOW_TAXI_DIR = \"home-work-1/nyc_taxi/yellow/\"\n",
    "GREEN_TAXI_DIR = \"home-work-1/nyc_taxi/green/\"\n",
    "PARQUET_DIRS = [YELLOW_TAXI_DIR, GREEN_TAXI_DIR]\n",
    "TAXI_ZONE_LOOKUP = \"s3://robot-dreams-source-data/home-work-1/nyc_taxi/taxi_zone_lookup.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47dfd1f-d254-4285-8ace-42006404e9ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def s3_files_search(s3, bucket_name, dirs, files_pathes=[], files_extension=None):\n",
    "    \n",
    "    while dirs:\n",
    "        \n",
    "        current_dir = dirs.pop(0)\n",
    "        dir_iterator = paginator.paginate(Bucket=bucket_name, Prefix=current_dir, Delimiter=\"/\")\n",
    "        \n",
    "        for dir_data in dir_iterator:\n",
    "            \n",
    "            if \"CommonPrefixes\" in dir_data:\n",
    "                current_subdirs = [\n",
    "                    subdir_dict['Prefix'] \n",
    "                    for subdir_dict in dir_data[\"CommonPrefixes\"]\n",
    "                ]\n",
    "                \n",
    "                dirs.extend(current_subdirs)\n",
    "\n",
    "            if \"Contents\" in dir_data:\n",
    "                current_files_pathes = [\n",
    "                    s3 + \"://\" + bucket_name + \"/\" + files_dict[\"Key\"] \n",
    "                    for files_dict in dir_data[\"Contents\"] \n",
    "                    if files_dict[\"Key\"].endswith(files_extension)\n",
    "                ]\n",
    "                \n",
    "                if current_files_pathes:\n",
    "                    files_pathes.extend(current_files_pathes)\n",
    "\n",
    "    return files_pathes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ecc78a-394c-4e30-b244-662a55515d04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_parquet_schema_summary(parquet_files_pathes):\n",
    "    \n",
    "    schema_summary = {}\n",
    "    \n",
    "    for parquet_file_path in parquet_files_pathes:\n",
    "        parquet_file_metadata = pq.ParquetFile(parquet_file_path)\n",
    "        parquet_file_schema = parquet_file_metadata.schema\n",
    "    \n",
    "        for col_idx in range(len(parquet_file_schema)):\n",
    "            column_name = parquet_file_schema.column(col_idx).name.lower()  \n",
    "            \n",
    "            logical_data_type = parquet_file_schema.column(col_idx).logical_type.type\n",
    "            if logical_data_type in [\"NONE\", \"UNKNOWN\"]:\n",
    "                logical_data_type = None\n",
    "            \n",
    "            physical_data_type = parquet_file_schema.column(col_idx).physical_type\n",
    "            \n",
    "            # Initialize schema entry if first time seen\n",
    "            if column_name not in schema_summary:\n",
    "                schema_summary[column_name] = {\n",
    "                    \"col_counter\": 1,\n",
    "                    \"dtypes\": [logical_data_type or physical_data_type]\n",
    "                }\n",
    "            else:\n",
    "                schema_summary[column_name][\"col_counter\"] += 1\n",
    "                dtype = logical_data_type or physical_data_type\n",
    "                if dtype not in schema_summary[column_name][\"dtypes\"]:\n",
    "                    schema_summary[column_name][\"dtypes\"].append(dtype)\n",
    "   \n",
    "    return schema_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e336900-5700-4efb-b458-9876900e9fa4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_df_cast_schema(parquet_files_schema, parquet_to_df_dtype_cast):\n",
    "    df_cast_schema = {}\n",
    "    \n",
    "    for col_name, col_data in parquet_files_schema.items():\n",
    "        if len(col_data['dtypes']) > 1:\n",
    "            df_cast_schema[col_name] = parquet_to_df_dtype_cast[col_data['dtypes'][0]]\n",
    "        \n",
    "    return df_cast_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6016e3a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def load_parquets_with_cast(parquet_files_pathes, df_cast_schema):\n",
    "    dfs = []\n",
    "    for parquet_file_path in parquet_files_pathes:\n",
    "        taxi_type = parquet_file_path.split(\"/\")[5]\n",
    "        df = spark.read.parquet(parquet_file_path)\n",
    "        for col_name in df_cast_schema.keys():\n",
    "            if col_name in df.columns:\n",
    "                df = df.withColumn(col_name, F.col(col_name).cast(df_cast_schema[col_name]))\n",
    "        df = df.withColumn(\"taxi_type\", F.lit(taxi_type))\n",
    "        dfs.append(df)\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05b6b3a-0507-45dc-8655-ac17e0fcf224",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inspect parquet files schema \n",
    "s3_boto = boto3.client(\"s3\")\n",
    "paginator = s3_boto.get_paginator(\"list_objects_v2\")\n",
    "parquet_files_pathes = s3_files_search(S3, BUCKET_NAME, PARQUET_DIRS, files_extension=\".parquet\")\n",
    "parquet_files_schema_summary = get_parquet_schema_summary(parquet_files_pathes)\n",
    "parquet_files_schema_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c7783d-470e-4bd3-882a-2cdaab8d15ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare list of fields that needs to be casted after read\n",
    "parquet_to_df_dtype_cast = {\n",
    "    \"INT32\": \"long\",\n",
    "    \"INT64\": \"long\",\n",
    "    \"DOUBLE\": \"double\",\n",
    "    \"BYTE_ARRAY\": \"string\",\n",
    "    \"BOOLEAN\": \"boolean\",\n",
    "    \"FLOAT\": \"float\",\n",
    "    \"FIXED_LEN_BYTE_ARRAY\": \"binary\",\n",
    "    \"TIMESTAMP\": \"timestamp\",\n",
    "    \"STRING\": \"string\"\n",
    "}\n",
    "\n",
    "df_cast_schema = get_df_cast_schema(parquet_files_schema_summary, parquet_to_df_dtype_cast)\n",
    "df_cast_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b06ffa-faae-44fb-b450-fee9211d9460",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load parquet files with cast\n",
    "dfs = load_parquets_with_cast(parquet_files_pathes, df_cast_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdcb368-b403-405c-bb79-ebe23c9e0657",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Union all the individual dfs\n",
    "raw_trips_df = reduce(lambda df1, df2: df1.unionByName(df2, allowMissingColumns=True), dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3cbb64-ceb9-4a9c-89ec-e93afa64a044",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add additional computations needed for further aggrgations\n",
    "raw_trips_df_enriched = raw_trips_df.withColumns({\n",
    "    \"trip_distance\": F.coalesce(F.col(\"trip_distance\"), F.lit(0)),\n",
    "    \"fare_amount\": F.coalesce(F.col(\"fare_amount\"), F.lit(0)),\n",
    "    \"pickup_datetime\": F.coalesce(F.col('tpep_pickup_datetime'), F.col('lpep_pickup_datetime')),\n",
    "    \"dropoff_datetime\": F.coalesce(F.col('tpep_dropoff_datetime'), F.col('lpep_dropoff_datetime')),\n",
    "    \"duration_min\": (F.unix_timestamp(\"dropoff_datetime\") - F.unix_timestamp(\"pickup_datetime\")) / 60,\n",
    "    \"pickup_hour\": F.hour(\"pickup_datetime\"),\n",
    "    \"pickup_day_of_week\": F.date_format(\"pickup_datetime\", \"E\"),\n",
    "    \"yellow_trip\": F.when(F.col(\"taxi_type\") == \"yellow\", 1).otherwise(0),\n",
    "    \"green_trip\": F.when(F.col(\"taxi_type\") == \"green\", 1).otherwise(0),\n",
    "    \"high_fare_trip\": F.when(F.col(\"fare_amount\") > 30, 1).otherwise(0)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daee7d33-738e-4abf-b144-5e6a8aaeb236",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply filtering conditions\n",
    "trips_df_filtered = raw_trips_df_enriched.filter(\n",
    "    (F.col(\"trip_distance\") > 0.1) &\n",
    "    (F.col(\"fare_amount\") > 2.0) &\n",
    "    (F.col(\"duration_min\") > 1.0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9092c8d1-1645-41d6-9543-778b480fce63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load taxi zone dictionary\n",
    "taxi_zone_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(TAXI_ZONE_LOOKUP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea89dcd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cast location id to be long type\n",
    "taxi_zone_df = taxi_zone_df.withColumn(\n",
    "    \"LocationID\", F.col(\"LocationID\").cast(\"long\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b3c900",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create temporary dfs to lookup pickup and dropoff zones\n",
    "pickup_zone_df = taxi_zone_df.select(\n",
    "    F.col(\"LocationID\").alias(\"PU_LocationID\"),\n",
    "    F.col(\"Zone\").alias(\"pickup_zone\")\n",
    ")\n",
    "\n",
    "dropoff_zone_df = taxi_zone_df.select(\n",
    "    F.col(\"LocationID\").alias(\"DO_LocationID\"),\n",
    "    F.col(\"Zone\").alias(\"dropoff_zone\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9c76b0-275d-4ae3-a6a1-e8f68c96ccae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Erich trips df with data from taxi zone dictionary\n",
    "trips_df_enriched = (\n",
    "    trips_df_filtered\n",
    "    .join(pickup_zone_df, trips_df_filtered[\"PULocationID\"] == pickup_zone_df[\"PU_LocationID\"], how=\"left\")\n",
    "    .join(dropoff_zone_df, trips_df_filtered[\"DOLocationID\"] == dropoff_zone_df[\"DO_LocationID\"], how=\"left\")\n",
    "    .drop(\"PU_LocationID\", \"DO_LocationID\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9317dbb8-b580-4f2a-9bfb-3d56745f519b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create aggregation and calculations for zone summary\n",
    "zone_summary = trips_df_enriched.groupby(\"pickup_zone\").agg(\n",
    "    F.count(\"*\").alias(\"total_trips\"),\n",
    "    F.avg('trip_distance').alias('avg_trip_distance'),\n",
    "    F.avg('total_amount').alias('avg_total_amount'),\n",
    "    F.avg('tip_amount').alias('avg_tip_amount'),\n",
    "    F.sum('yellow_trip').alias('yellow_trips'),\n",
    "    F.sum('green_trip').alias('green_trips'),\n",
    "    F.max('trip_distance').alias('max_trip_distance'),\n",
    "    F.min('tip_amount').alias('min_tip_amount'),\n",
    ")\n",
    "\n",
    "zone_summary = zone_summary.withColumns({\n",
    "    \"yellow_share\": F.col(\"yellow_trips\") / F.col(\"total_trips\"),\n",
    "    \"green_share\": F.col(\"green_trips\") / F.col(\"total_trips\")\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec47870c-1273-44ef-bd35-759aea80cc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create aggregation and calculations for by weekday, zone summary\n",
    "zone_days_statistic = trips_df_enriched.groupby([\"pickup_day_of_week\", \"pickup_zone\"]).agg(\n",
    "    F.count(\"*\").alias(\"total_trips\"),\n",
    "    F.sum(\"high_fare_trip\").alias(\"high_fare_trips\")\n",
    ")\n",
    "\n",
    "zone_days_statistic = zone_days_statistic.withColumn(\n",
    "    \"high_fare_share\", F.col(\"high_fare_trips\") / F.col(\"total_trips\")\n",
    ")\n",
    "\n",
    "zone_days_statistic = zone_days_statistic.filter(F.col(\"pickup_day_of_week\") == \"Mon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b041ed",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Save files\n",
    "date_str = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "output_path_zone_s = f\"s3://dhalahan-emr-studio/zone_statistic/{date_str}/\"\n",
    "output_path_zone_d = f\"s3://dhalahan-emr-studio/zone_days_statistic/{date_str}/\"\n",
    "zone_summary.coalesce(1).write.mode(\"overwrite\").parquet(output_path_zone_s)\n",
    "zone_days_statistic.coalesce(1).write.mode(\"overwrite\").parquet(output_path_zone_d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
